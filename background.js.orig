<<<<<<< HEAD
let currentTabId = null;
let pageDataResolver = null;

function delay(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}
function randomDelay(minMs, maxMs) {
  return delay(minMs + Math.floor(Math.random() * (maxMs - minMs)));
}
function downloadFile(options) {
  return new Promise(resolve => chrome.downloads.download(options, resolve));
}
function fetchBlob(url) {
  return fetch(url).then(r => r.blob());
}
function waitForTabLoad(tabId) {
  return new Promise(resolve => {
    chrome.tabs.onUpdated.addListener(function listener(updatedTabId, changeInfo) {
      if (updatedTabId === tabId && changeInfo.status === "complete") {
        chrome.tabs.onUpdated.removeListener(listener);
        resolve();
      }
=======
//--------------------------------------
// CONFIG
//--------------------------------------
const page_delay_min = 3000;
const page_delay_range = 2000;
const image_delay_min = 100;
const image_delay_range = 50;
const TWEET_CACHE_LIMIT = 30;

//--------------------------------------
// STATE
//--------------------------------------
let isPaused = false;
let isRunning = false;
let pageDataResolver = null;
let currentTabId = null;
let totalPages = null;
let finalPage = null;
let maxPages = null;
const postIdToPage = {};
const scrapeStateKey = 'sa_scraper_state';
const tweetCache = new Map();

//--------------------------------------
// HELPERS
//--------------------------------------
function delayRandom(min, range) {
  const ms = min + Math.floor(Math.random() * range);
  return new Promise(res => setTimeout(res, ms));
}

function waitWithTimeout(executor, timeout = 10000, onTimeout) {
    return new Promise((resolve, reject) => {
        let settled = false;

        const timer = setTimeout(() => {
            if (settled) return;
            settled = true;
            if (onTimeout) onTimeout();
            reject(new Error(`Timeout after ${timeout} ms`));
        }, timeout);

        executor(
            (value) => {
                if (settled) return;
                settled = true;
                clearTimeout(timer);
                resolve(value);
            },
            (err) => {
                if (settled) return;
                settled = true;
                clearTimeout(timer);
                reject(err);
            }
        );
    });
}

function waitForPageLoad(tabId, timeout = 30000) {
    return waitWithTimeout((resolve) => {
        function listener(details) {
            if (details.tabId === tabId && details.frameId === 0) {
                chrome.webNavigation.onCompleted.removeListener(listener);
                log(`[waitForPageLoad] Page load completed for tab ${tabId}.`);
                resolve();
            }
        }
        chrome.webNavigation.onCompleted.addListener(listener);
    }, timeout, () => {
        chrome.webNavigation.onCompleted.removeListener(listener);
        log(`[waitForPageLoad] Timeout waiting for page load on tab ${tabId}`);
    });
}

async function scrapeInTab(pageUrl, selector_to_await, scriptFile) {
    await new Promise(resolve => chrome.tabs.update(currentTabId, { url: pageUrl }, resolve));
    log(`[scrapeThread] Tab updated, waiting for ${pageUrl} page load...`);
    await waitForPageLoad(currentTabId);
    await new Promise(r => setTimeout(r, 200)); // give JS context time to reset; hoping this helps to not inject script twice.
    log(`[scrapeThread] ${pageUrl} load complete.`);

    const pageDataPromise = waitWithTimeout((resolve) => {
          pageDataResolver = resolve;
      }, 20000, () => {
          isPaused = true;
          chrome.runtime.sendMessage({
              type: 'status',
              text: 'ðŸ›‘ Error: Timeout waiting for page data â€” paused.'
          });
    });

    // Now inject script
    log(`Will attempt to inject script into ${pageUrl}...`);
    await new Promise((resolve, reject) => {
      chrome.tabs.executeScript(currentTabId, { file: scriptFile, runAt: 'document_idle' }, res => {
        if (chrome.runtime.lastError) {
          log(`âŒ Failed to inject ${scriptFile}: ${chrome.runtime.lastError.message}`);
          reject(chrome.runtime.lastError);
          return;
        }
        log(`ðŸ“¥ ${scriptFile} injected`);
        resolve(res);
      });
    });
  return await pageDataPromise;
}

function fetchWithTimeout(url, options = {}, timeout = 5000) {
    const controller = new AbortController();
    return waitWithTimeout((resolve, reject) => {
        const signal = controller.signal;
        fetch(url, { ...options, signal })
            .then(resolve)
            .catch(reject);
    }, timeout, () => controller.abort());
}

async function downloadBlob(blob, folder, filename) {
  const url = URL.createObjectURL(blob);
  return new Promise((resolve, reject) => {
    chrome.downloads.download({
      url,
      filename: `${folder}/${filename}`,
      saveAs: false,
      conflictAction: 'overwrite'
    }, (downloadId) => {
      // Delay revoke to ensure download start
      setTimeout(() => URL.revokeObjectURL(url), 200);
      if (chrome.runtime.lastError) reject(chrome.runtime.lastError);
      else resolve(downloadId);
>>>>>>> 6e7b0fa (Initial commit; Scraping generally works but tweet text isn't being awaited, and other issues with the two tweet code paths not filling in fields.)
    });
  });
}

<<<<<<< HEAD
async function scrapeThread(startUrl) {
  const threadIdMatch = startUrl.match(/threadid=(\d+)/);
  if (!threadIdMatch) {
    console.error("Invalid SA thread URL.");
    return;
  }

  const threadId = threadIdMatch[1];
  const maxPages = 5;
  const threadDir = `SA_Thread_${threadId}`;
  const baseUrl = `https://forums.somethingawful.com/showthread.php?threadid=${threadId}&pagenumber=`;

  const [tab] = await new Promise(resolve =>
    chrome.tabs.query({ active: true, currentWindow: true }, resolve)
  );

  currentTabId = tab.id;

  for (let pageNum = 1; pageNum <= maxPages; pageNum++) {
    const url = `${baseUrl}${pageNum}`;
    console.log(`âž¡ï¸ Navigating to page ${pageNum}: ${url}`);
    chrome.tabs.update(currentTabId, { url });

    await waitForTabLoad(currentTabId);

    await new Promise((resolve, reject) => {
      chrome.tabs.executeScript(currentTabId, {
        file: "scraper.js",
        runAt: "document_idle", // or "document_end"
        allFrames: false
      }, result => {
        if (chrome.runtime.lastError) {
          console.error("âŒ Failed to inject scraper.js:", chrome.runtime.lastError);
          reject(chrome.runtime.lastError);
        } else {
          console.log("ðŸ“¥ scraper.js injected");
          resolve(result);
        }
      });
    });

    const pageData = await new Promise(resolve => {
      pageDataResolver = resolve;
    });

    if (!pageData || pageData.length === 0) {
      console.log(`âœ… No posts found on page ${pageNum}, stopping.`);
      break;
    }

    const htmlParts = pageData.map(post => post.html).join('\n');
    const filename = `${threadDir}/page${String(pageNum).padStart(4, '0')}.html`;

    const htmlBlob = new Blob([
      `<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Thread ${threadId} - Page ${pageNum}</title>
  <style>
    body { font-family: sans-serif; padding: 40px; max-width: 900px; margin: auto; }
    .postbox { margin-bottom: 3em; border-bottom: 1px solid #ccc; padding-bottom: 1em; }
    .meta { font-size: 0.9em; color: #666; margin-bottom: 5px; }
    img { max-width: 100%; }
  </style>
</head>
<body>
${htmlParts}
</body>
</html>`.trim()
    ], { type: "text/html" });

    const htmlURL = URL.createObjectURL(htmlBlob);
    await downloadFile({ url: htmlURL, filename, saveAs: false });
    console.log(`ðŸ’¾ Saved HTML: ${filename}`);

    for (const post of pageData) {
      for (const img of post.images) {
        try {
          const imgBlob = await fetchBlob(img.url);
          const blobUrl = URL.createObjectURL(imgBlob);
          const imgFile = `${threadDir}/images/${img.filename}`;
          await downloadFile({ url: blobUrl, filename: imgFile, saveAs: false });
          console.log(`ðŸ–¼ Saved image: ${imgFile}`);
          await delay(250 + Math.random() * 300);
        } catch (err) {
          console.warn(`âš ï¸ Failed to download: ${img.url}`);
        }
      }
    }

    chrome.runtime.sendMessage({
      action: 'status',
      msg: `âœ… Page ${pageNum} archived`
    });

    await randomDelay(5000, 10000);
  }

  console.log("ðŸŽ‰ Finished thread scraping.");
  currentTabId = null;
  pageDataResolver = null;
}

chrome.runtime.onMessage.addListener((msg, sender) => {
  console.log("[BG] Received message", msg, sender);
  if (msg.action === "start-scrape") {
    if (!msg.url) return;
    console.log("[BG] Starting scrapeâ€¦");
    scrapeThread(msg.url);
  }

  if (msg.action === "page-data" && pageDataResolver) {
    console.log(`[BG] Got page data (sender tab: ${sender.tab?.id})`);
    pageDataResolver(msg.data);
    pageDataResolver = null;
  }
});
=======
function log(msg) {
  console.log(`[SA Scraper] ${msg}`);
  chrome.runtime.sendMessage({ type: 'status', text: msg });
}

async function waitWhilePaused() {
  while (isPaused) {
    log("[waitWhilePaused] Paused; waiting 1s...");
    await new Promise(r => setTimeout(r, 1000));
  }
}

function storageLocal(action, keyOrObj) {
  return new Promise((resolve, reject) => {
    chrome.storage.local[action](keyOrObj, result => {
      if (chrome.runtime.lastError) reject(chrome.runtime.lastError);
      else resolve(result);
    });
  });
}

//--------------------------------------
// TWEETS
//--------------------------------------
async function fetchTweetFromArchive(tweetUrl) {
  // Step 1: Find the most recent capture timestamp via Wayback CDX API
  const cdxApi = `https://web.archive.org/cdx/search/cdx?url=${encodeURIComponent(tweetUrl)}&output=json&fl=timestamp,original&filter=statuscode:200&limit=1&collapse=digest`;
  const resp = await fetchWithTimeout(cdxApi, {}, 5000);
  const arr = await resp.json();
  if (!Array.isArray(arr) || arr.length < 2) throw new Error("No archive snapshot found for tweet");

  const [header, snap] = arr;
  const [timestamp] = snap;
  const waybackUrl = `https://web.archive.org/web/${timestamp}id_/${tweetUrl}`;

  // Step 2: Fetch the archived tweet HTML
  const wbResp = await fetchWithTimeout(waybackUrl, {}, 5000);
  const html = await wbResp.text();
  const parser = new DOMParser();
  const doc = parser.parseFromString(html, "text/html");

  // Step 3: Get tweet content
  const mainTweetEl = doc.querySelector('p.tweet-text, div.tweet-text');
  const mainHTML = mainTweetEl ? mainTweetEl.innerHTML : '';

  const timeEl = doc.querySelector('.tweet-timestamp');
  const tweetTime = timeEl?.getAttribute('title') || "";
  const authorName = doc.querySelector('.FullNameGroup .fullname')?.textContent.trim() || "";
  const authorHandle = doc.querySelector('.username.u-dir b')?.textContent.trim() || "";

  const quoteTweetEls = Array.from(doc.querySelectorAll('.QuoteTweet .QuoteTweet-text'));
  const quoteHTML = quoteTweetEls.map(el => el.innerHTML).join('<hr>') || '';

  const replyEls = Array.from(doc.querySelectorAll('p.TweetTextSize.js-tweet-text.tweet-text')).slice(0, 5);;
  const repliesHTML = replyEls.length ? replyEls.map(el => el.innerHTML) : [];

  return { authorName, authorHandle, tweetTime, mainHTML, quoteHTML, repliesHTML, tweetUrl };
}

// Cache keyed by tweet URL
function cacheTweet(url, data) {
  if (tweetCache.size >= TWEET_CACHE_LIMIT) {
    // Remove oldest
    const oldestKey = tweetCache.keys().next().value;
    tweetCache.delete(oldestKey);
  }
  tweetCache.set(url, data);
}

async function tryFetchAndCache(fetchFn, tweetUrl) {
  try {
    const data = await fetchFn(tweetUrl);
    if (data) {
      tweetCache.set(tweetUrl, data);
      return data;
    }
  } catch (err) {
    console.warn(`Failed ${fetchFn} for ${tweetUrl}: ${err.message}`);
  }
  return null;
}

async function getTweetData(tweetUrl) {
  if (tweetCache.has(tweetUrl)) return tweetCache.get(tweetUrl);

  // Try archive.org first
  let data = await tryFetchAndCache(fetchTweetFromArchive, tweetUrl);
  if (data) return data;

  // Fallback: live scrape tweet in controlled tab
  data = await tryFetchAndCache( url => scrapeInTab(url,'div[data-testid="tweetText"]', 'tweet-scraper.js'), tweetUrl);
  if (data) return data;

  // Return minimal fallback placeholder
  const fallbackData = {
    author: '',
    tweetTime: '',
    mainHTML: `<a href="${tweetUrl}" target="_blank">${tweetUrl}</a>`,
    repliesHTML: []
  };
  tweetCache.set(tweetUrl, fallbackData);
  return fallbackData;
}

function generateLocalTweetBox({ authorName, authorHandle, tweetTime, mainHTML, quoteHTML, replies, tweetUrl }) {
  const repliesHTML = `<div class="tweet-replies">${replies?.map(r => `<div class="tweet-reply">${r}</div>`).join('')}</div>`;
  const quoteBlock = `<div class="tweet-quote">${quoteHTML}</div>`

  return `
    <div class="local-tweet">
      <div class="tweet-header">
        <span class="tweet-author">${authorName} <span class="tweet-handle">@${authorHandle}</span></span>
        <a href="${tweetUrl}" target="_blank" title="${tweetTime}">
          <span class="tweet-time">${tweetTime || "Unknown time"}</span>
        </a>
      </div>
      <div class="tweet-main">${mainHTML}</div>
      ${quoteBlock}
      ${repliesHTML}
    </div>
  `;
}
//--------------------------------------
// CORS-SAFE IMAGE FETCH FROM BACKGROUND
//--------------------------------------
async function fetchAndDownloadImage(img, folder) {
  try {
    // Background context bypasses content script CORS limits if permission is granted in manifest
    log(`Fetching image: ${img.url}`);
    const response = await fetchWithTimeout(img.url, { credentials: "include" }, 8000);
    if (!response.ok) throw new Error(`HTTP ${response.status}`);
    const blob = await response.blob();
    if (blob.size > 0) { await downloadBlob(blob, folder + '/images', img.filename); }
    else { throw "Zero size blob." }
    log(`Saved image: images/${img.filename}`);
  } catch (error) {
    log(`ðŸ›‘ Error fetching image ${img.url}: ${error.message}`);
    isPaused = true;
    chrome.runtime.sendMessage({
      type: 'status',
      text: `ðŸ›‘ Error fetching image: ${img.url} â€” ${error.message}. Archiving paused.`
    });
    throw error;
  }
}

// Fallback-aware downloader for grouped media sources (e.g., multiple video formats)
async function fetchMediaGroupWithFallback(sources, folder) {
  for (const src of sources) {
    try {
      await fetchAndDownloadImage(src, folder);
      // If fetch succeeded, no need to try others
      return;
    } catch (err) {
      // Log error but continue trying others
      log(`âš ï¸ Attempt failed for ${src.url}: ${err.message}`);
    }
  }
  // If none succeeded, pause and throw
  log(`ðŸ›‘ All media sources failed for ${sources[0].filename}. Pausing scrape.`);
  isPaused = true;
  chrome.runtime.sendMessage({
    type: 'status',
    text: `ðŸ›‘ All media sources failed for ${sources[0].filename}. Archiving paused.`
  });
  throw new Error(`All media sources failed for ${sources[0].filename}`);
}

async function fetchImagesWithPoolWithFallback(images, folder, concurrency = 4) {
  // Group images by base filename without extension to group multiple formats
  const groups = {};

  function getBaseName(filename) {
    return filename.toLowerCase().replace(/\.(mp4|webm|jpg|jpeg|png|gif)$/i, '');
  }

  images.forEach(img => {
    const base = getBaseName(img.filename);
    if (!groups[base]) groups[base] = [];
    groups[base].push(img);
  });

  const groupKeys = Object.keys(groups);
  let index = 0;

  async function worker() {
    while (index < groupKeys.length && !isPaused) {
      const currentGroup = groups[groupKeys[index++]];
      await fetchMediaGroupWithFallback(currentGroup, folder);
      await delayRandom(image_delay_min, image_delay_range);
    }
  }

  const workers = [];
  for (let i = 0; i < concurrency; i++) workers.push(worker());
  await Promise.all(workers);
}

//--------------------------------------
// MESSAGE HANDLER
//--------------------------------------
chrome.runtime.onMessage.addListener((msg, sender, sendResponse) => {
  if (msg.type === 'page-data' && pageDataResolver) {
    log(`[onMessage] Resolving pageDataResolver with page-data (${msg ? 'OK' : 'null'})`);
    if (msg.lastPageNumber && !isNaN(msg.lastPageNumber)) {
      totalPages = msg.lastPageNumber;
    }
    pageDataResolver(msg);
    pageDataResolver = null;
  }
  if (msg.command === 'start-scrape') {
    if (msg.maxPages && !isNaN(msg.maxPages)) {
      maxPages = msg.maxPages;
    }
    if (isPaused) isPaused = false;
    log("[onMessage] Starting scrapeThread.");
    scrapeThread();
  }
  if (msg.type === 'scraping-error') {
    isPaused = true;
    log(`ðŸ›‘ Error reported from content script: ${msg.message} â€” paused.`);
    chrome.runtime.sendMessage({ type: 'status', text: 'ðŸ›‘ Error: ' + msg.message + ' (paused)' });
  }
  if (msg.command === 'getStatusAndResume') {
    chrome.tabs.query({ active: true, currentWindow: true }, (tabArr) => {
      const tab = tabArr[0];
      const threadId = tab?.url.includes('threadid=')
        ? (tab.url.match(/threadid=(\d+)/) || [])[1]
        : null;

      if (!threadId) {
        sendResponse({ isRunning, isPaused, resumePage: null });
        return;
      }
      const key = `${scrapeStateKey}_${threadId}`;
      storageLocal('get', key).then(val => {
        const resumePage = val[key] ?? null;
        sendResponse({ isRunning, resumePage });
      });
    });
    return true; // async sendResponse
  }

  if (msg.command === 'pause') isPaused = true;

  if (msg.command === 'resume') {
      isPaused = false;
      log("[onMessage] Resume requested.");

      // If not currently running, start scrapeThread() in resume mode
      if (!isRunning) {
          log("[onMessage] Starting scrapeThread() in resume mode.");
          scrapeThread();
      }
  }
  return false;  // close the channel; no async sendResponse()
});

//--------------------------------------
// MAIN SCRAPE FUNCTION
//--------------------------------------
async function scrapeThread() {
  if (isRunning) {
    log("Scrape already in progress.");
    return;
  }
  isRunning = true;
  log("Scraper thread started.");

  const [tab] = await new Promise(r => chrome.tabs.query({ active: true, currentWindow: true }, r));
  log(`Found active tab: id=${tab?.id}, url=${tab?.url}`);
  if (!tab || !tab.url.includes('showthread.php?threadid=')) {
    log('âŒ Not on a Something Awful thread page.');
    isRunning = false;
    return;
  }
  currentTabId = tab.id;

  const threadIdMatch = tab.url.match(/threadid=(\d+)/);
  const threadId = threadIdMatch ? threadIdMatch[1] : 'unknown';
  log(`Thread ID: ${threadId}`);
  const threadDir = `SA_Thread_${threadId}`;
  const baseUrl = tab.url.split('&pagenumber=')[0] + '&pagenumber=';

  const key = `${scrapeStateKey}_${threadId}`;
  const resumeStateRaw = await storageLocal('get', key);
  const resumePage = (resumeStateRaw || {})[key] || 0;
  log(`Loaded resume page: ${JSON.stringify(resumePage)}`);

  let pageNum = 1;
  if (resumePage > 0 && resumePage < totalPages) {
    pageNum = resumePage;
    finalPage = Math.min( pageNum + maxPages, totalPages);
    log(`ðŸ”„ Resuming thread ${threadId} from page ${pageNum}, will scrape ${maxPages} more pages, stopping at ${finalPage}`);
  } else {
    finalPage = maxPages;
    log(`Starting fresh scrape, will scrape ${maxPages} pages total.`);
  }

  chrome.runtime.sendMessage({
    type: 'progressUpdate',
    page: pageNum,
    max: finalPage
  });

  for (; pageNum <= finalPage; pageNum++) {
    log(`[scrapeThread] Starting page ${pageNum}`);
    await waitWhilePaused();

    const pageUrl = baseUrl + pageNum;
    log(`âž¡ï¸ Navigating to page ${pageNum}: ${pageUrl}`);

    const pageData = await scrapeInTab(pageUrl, 'table.post',  'scraper.js');
    log(`[scrapeThread] pageData posts count: ${pageData?.posts?.length ?? 'no posts property'}`);

    if (!pageData || !pageData.posts || !pageData.posts.length) {
      log(`ðŸ›‘ ERROR: No posts found on page ${pageNum}. Archiving paused.`);
      isPaused = true;
      chrome.runtime.sendMessage({ type: 'status', text: 'ðŸ›‘ Error: No posts found â€“ paused.' });
      break;
    }
    if (isPaused) {
      log("â¸ Scraping paused, restarting this loop iteration.");
      pageNum--;
      continue;
    }
    // Create a mapping to find page numbers containing a post by ID.
    for (const post of pageData.posts) {
      postIdToPage[post.postId] = pageNum;
    }

    // Save HTML for the page
    const htmlParts = pageData.posts.map(p => p.html).join('\n');
    // Wrap in a DOMParser so we can safely find quote links:
    const parser = new DOMParser();
    const doc = parser.parseFromString(htmlParts, 'text/html');

    doc.querySelectorAll('a.quote_link').forEach(a => {
      const m = a.getAttribute('href').match(/postid=(\d+)/);
      if (m) {
        const pid = m[1];
        const pageForPost = postIdToPage[pid];
        if (pageForPost) {
          a.href = `page${String(pageForPost).padStart(4, '0')}.html#post${pid}`;
        } else {
          // Use the original HTML href, not the resolved a.href
          const origHref = a.getAttribute('href'); // e.g., "/showthread.php?goto=post&postid=503753878#post503753878"
          a.href = "https://forums.somethingawful.com" + origHref;
        }
      }
    });

    // Tweet replacement
    for (const target of [
      ...doc.querySelectorAll('a[data-archive-later="tweet"]'),
      ...doc.querySelectorAll('blockquote.twitter-tweet')
    ]) {
      let tweetUrl = null;
      let replaceNode = target;

      if (target.matches('a[data-archive-later="tweet"]')) {
        tweetUrl = target.href;
      } else {
        const link = target.querySelector('a[href*="twitter.com/"][href*="/status/"]');
        if (!link) continue;
        tweetUrl = link.href;
        replaceNode = target.parentElement; // Replace whole preview container
      }
      if (!tweetUrl || !/twitter\.com\/[^/]+\/status\/\d+/.test(tweetUrl)) {
          log(`Skipping non-status Twitter URL: ${tweetUrl}`);
          continue;
      }

      const tweetData = await getTweetData(tweetUrl); // your cache+fetch

      const tweetBoxHTML = generateLocalTweetBox(tweetData);

      const wrapper = doc.createElement('div');
      wrapper.innerHTML = tweetBoxHTML;
      replaceNode.replaceWith(...wrapper.childNodes);
    }

    const htmlPartsFixed = doc.body.innerHTML;

    const htmlContent = `
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Thread ${threadId} â€” Page ${pageNum}</title>
  <style>
    body { font-family: sans-serif; max-width: 85%; margin: auto; padding: 40px; line-height: 1.4; }
    .postbox { margin-left: 12px; border-bottom: 1px solid #888 }
    .meta { font-size: 0.9em; color: #666; margin-bottom: 0.5em; }
    img { max-width: 100%; }
    .postbox .body {
      padding: 12px 6px;
      margin: 0;
    }
    /* --- Dark mode quote box & spoiler styling --- */
    body {
      background-color: #090a0d;
      color: #AAA;
    }
    .bbc-block {
      border-radius: 10px;
      border: 1px solid #343434;
      background-color: #1b202b;
      margin: 12px 22px;
      padding: 2px 18px;
      box-shadow: none;
    }
    .bbc-block h4, .bbc-block h5 { color: #999; }
    .bbc-block h5 { border-color: #343434; }
    .bbc-block blockquote { color: #A5B7CF; }
    .quoteInner { overflow-y: hidden; text-wrap: wrap; }
    .bbc-spoiler:hover, .bbc-spoiler.reveal, .bbc-spoiler.stay {
      background-color: #1c212a;
      color: #CCC;
    }
    .bbc-spoiler blockquote, blockquote .bbc-spoiler,
    blockquote .bbc-spoiler li, .bbc-spoiler blockquote li {
      background-color: #304867;
      color: rgba(48, 72, 103, 0);
    }
    .postbox.seen1 {
      background-color: #151a2b !important;
    }
    .postbox.seen2 {
      background-color: #0d111e !important;
}

    /* --- Minimal timg thumbnail support --- */
    .timg_container {
      position: relative;
      display: inline-block;
      cursor: pointer;
      overflow: hidden;
      max-width: 300px; /* Thumbnail size; adjust as desired */
      max-height: 300px;
      transition: max-width .2s, max-height .2s;
      background: #eee;
    }
    .timg_container.expanded {
      max-width: none;
      max-height: none;
      background: transparent;
      z-index: 10;
    }
    .timg_container img.timg {
      display: block;
      width: 100%;
      height: auto;
      transition: box-shadow .2s;
      box-shadow: 0 1px 5px rgba(0,0,0,0.1);
    }
    .timg_container.expanded img.timg {
      box-shadow: 0 2px 12px rgba(50,50,90,.2);
    }

    .timg_container .note {
      position: absolute;
      top: 4px; left: 4px;
      font-size: 11px;
      background: rgba(28,56,120,.75);
      color: #fff;
      padding: 3px 14px 3px 24px;
      border-radius: 8px;
      opacity: 0.9;
      z-index: 20;
      user-select: none;
      display: none;
      cursor: pointer;
      /* icon using an SVG background as replacement for SA's PNG */
      background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><polygon points="4,2 12,8 4,14" style="fill:white;"/></svg>');
      background-repeat: no-repeat;
      background-position: 4px center;
    }
    .timg_container.expanded .note {
      /* reverse arrow icon for expanded */
      background-image: url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><polygon points="12,2 4,8 12,14" style="fill:white;"/></svg>');
    }
    .timg_container:hover .note,
    .timg_container:focus .note {
      display: block;
    }
    /* Local tweet embed */
    .local-tweet {
      font-family: "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      border: 1px solid #e1e8ed;
      border-radius: 12px;
      padding: 12px 16px;
      margin: 12px 0;
      background-color: #fff;
      color: #0f1419;
      max-width: 550px;
    }

    .local-tweet .tweet-header {
      display: flex;
      align-items: center;
      margin-bottom: 6px;
      font-size: 14px;
      color: #536471;
    }

    .local-tweet .tweet-author {
      font-weight: bold;
      margin-right: 6px;
      color: #0f1419;
    }

    .local-tweet .tweet-time {
      margin-left: auto;
      font-size: 13px;
      color: #536471;
    }

    .local-tweet .tweet-main {
      font-size: 15px;
      line-height: 1.4;
      white-space: pre-wrap;
    }

    .local-tweet .tweet-replies {
      border-top: 1px solid #e1e8ed;
      margin-top: 8px;
      padding-top: 8px;
    }

    .local-tweet .tweet-reply {
      padding: 6px 0;
      font-size: 14px;
      color: #0f1419;
      border-bottom: 1px solid #f0f0f0;
    }

    .local-tweet .tweet-reply:last-child {
      border-bottom: none;
    }
  </style>
</head>
<body>
${htmlPartsFixed}
<script>
document.addEventListener('DOMContentLoaded', function () {
  document.querySelectorAll('.timg_container .note').forEach(function(note) {
    note.addEventListener('click', function(e) {
      e.preventDefault();
      var container = note.closest('.timg_container');
      if (!container) return;
      if (container.classList.contains('expanded')) {
        // Collapse
        container.classList.remove('expanded');
        note.classList.remove('expanded');
      } else {
        // Expand
        container.classList.add('expanded');
        note.classList.add('expanded');
        // Auto-scroll into view if needed
        var img = container.querySelector('img.timg');
        if (img) {
          var rect = img.getBoundingClientRect();
          if (rect.bottom > window.innerHeight || rect.top < 0) {
            img.scrollIntoView({behavior: 'smooth', block: 'center'});
          }
        }
      }
    });
  });
});
</script>
</body>
</html>`.trim();

    log(`[scrapeThread] Saving HTML for page ${pageNum}`);
    await downloadBlob(new Blob([htmlContent], { type: 'text/html' }), threadDir, `page${String(pageNum).padStart(4, '0')}.html`);
    log(`ðŸ’¾ Saved HTML page${String(pageNum).padStart(4, '0')}.html`);

    // *** CORS-SAFE IMAGE DOWNLOAD IN BACKGROUND ***
    const allImages = pageData.posts.flatMap(p => p.images || []);
    log(`[scrapeThread] Starting image downloads for ${allImages.length} images.`);
    await fetchImagesWithPoolWithFallback(allImages, threadDir, 4);
    log(`[scrapeThread] Image downloads complete for page ${pageNum}`);

    log(`âœ… Archived page ${pageNum}`);
    await storageLocal('set', { [`${scrapeStateKey}_${threadId}`]: pageNum+1 });

    if (pageNum < finalPage) {
      chrome.runtime.sendMessage({ type: 'progressUpdate', page: pageNum+1, max: finalPage });
      log(`[scrapeThread] Waiting before next page...`);
      await delayRandom(page_delay_min, page_delay_range);
    }
  }

  log('ðŸŽ‰ Finished scraping thread.');
  currentTabId = null;
  pageDataResolver = null;
  await storageLocal('remove', [`${scrapeStateKey}_${threadId}`]);
  isRunning = false;
}

// Remove/block images/media from the original network traffic, since we'll need a separate fetch to scrape each one anyway.
chrome.webRequest.onBeforeRequest.addListener(
  (details) => {
    // Only block if blocking is enabled and tabId matches
    if (currentTabId !== null && details.tabId === currentTabId) {
      return { cancel: true };
    }
    // Otherwise do not block
  },
  {
    // Filter for all URLs, but you can restrict to SA domains or relevant origins
    urls: ["<all_urls>"],
    types: ["image", "media", "object", "other"] // block images, videos, and possibly other media types
  },
  ["blocking"]
);

>>>>>>> 6e7b0fa (Initial commit; Scraping generally works but tweet text isn't being awaited, and other issues with the two tweet code paths not filling in fields.)

